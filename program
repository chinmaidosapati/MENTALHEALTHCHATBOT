!pip install chromadb
import os
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain_community.document_loaders import DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq  # ✅ Make sure this is installed via pip

# Initialize the LLM
def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model_name="llama3-70b-8192"
    )
    return llm

# Create vector database using PDFs
def create_vector_db():
    loader = DirectoryLoader("/content/data", glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_db = Chroma.from_documents(texts, embeddings, persistent_directory='./chroma_db')
    vector_db.persist()
    print("ChromaDB created and data saved.")
    return vector_db

# Setup QA chain
def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_template = """You are a compassionate mental health bot. Respond through the given data:

{context}

User: {question}
"""
    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT},
    )
    return qa_chain

# Main function
def main():
    print("Initializing Chatbot...")
    llm = initialize_llm()
    db_path = "./chroma_db"

    if not os.path.exists(db_path):
        vector_db = create_vector_db()
    else:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)  # ✅ FIXED

    qa_chain = setup_qa_chain(vector_db, llm)

    while True:
        query = input("\nHuman: ")
        if query.lower() == "exit":
            print("Chatbot: Take care of yourself. Goodbye!")
            break
        response = qa_chain.run(query)
        print("Chatbot:", response)


if __name__ == "__main__":
    main()
